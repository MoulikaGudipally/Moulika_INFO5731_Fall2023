{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoulikaGudipally/Moulika_INFO5731_Fall2023/blob/main/Gudipally_Moulika_exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAzh1U0sE5I5"
      },
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpgvZQdRE-HV"
      },
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LmNR3kw9-pa"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "The research question centers on exploring the impact of remote work policies on employee productivity, job satisfaction, and overall well-being, while also identifying key factors influencing these outcomes. To address this question comprehensively, a multifaceted approach to data collection is necessary. Employee surveys will be conducted, encompassing quantitative assessments of productivity levels, job satisfaction, and stress levels, supplemented by qualitative insights gathered through open-ended questions. Employer data will be obtained to understand the specifics of remote work policies and practices in place. Performance metrics, such as project completion rates and meeting deadlines, will be analyzed alongside well-being metrics to gauge the effects on mental health and work-life balance. Qualitative interviews with employees and employers will delve deeper into their experiences. The sample size will be determined through power analysis, considering population size and desired confidence levels. Data will be meticulously collected, stored securely, and analyzed statistically to provide a comprehensive answer to the research question while adhering to ethical and data protection regulations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpWOgjHi9-pa",
        "outputId": "4af7c9df-5a20-46d6-eada-e9494accb23d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data collection and saving complete.\n",
            "     Employee_ID  Productivity_Before  Productivity_After  Job_Satisfaction  \\\n",
            "0              1             2.960848            1.080756                 3   \n",
            "1              2             0.705651            1.646757                 1   \n",
            "2              3             3.298833            0.439299                 3   \n",
            "3              4             1.997653            4.537735                 4   \n",
            "4              5             2.488921            3.105447                 5   \n",
            "..           ...                  ...                 ...               ...   \n",
            "995          996             4.555949            0.715716                 2   \n",
            "996          997             3.159736            2.326840                 2   \n",
            "997          998             1.820273            2.402000                 5   \n",
            "998          999             2.619769            1.855279                 3   \n",
            "999         1000             4.807654            0.591975                 1   \n",
            "\n",
            "     Well_Being  \n",
            "0      6.889099  \n",
            "1      3.429864  \n",
            "2      2.191100  \n",
            "3      1.221864  \n",
            "4      7.691201  \n",
            "..          ...  \n",
            "995    6.762216  \n",
            "996    6.722046  \n",
            "997    7.355717  \n",
            "998    7.381261  \n",
            "999    4.291903  \n",
            "\n",
            "[1000 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "data_list = []\n",
        "\n",
        "# Generate synthetic data for 1000 employees\n",
        "for employee_id in range(1, 1001):\n",
        "    productivity_before = random.uniform(0, 5)  # Assuming a scale from 0 to 5\n",
        "    productivity_after = random.uniform(0, 5)\n",
        "    job_satisfaction = random.randint(1, 5)  # Assuming a 1-5 Likert scale\n",
        "    well_being = random.uniform(0, 10)  # Assuming a scale from 0 to 10\n",
        "\n",
        "    data_list.append({\n",
        "        'Employee_ID': employee_id,\n",
        "        'Productivity_Before': productivity_before,\n",
        "        'Productivity_After': productivity_after,\n",
        "        'Job_Satisfaction': job_satisfaction,\n",
        "        'Well_Being': well_being\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the list of dictionaries\n",
        "data = pd.DataFrame(data_list)\n",
        "\n",
        "# Save the synthetic data to a CSV file\n",
        "data.to_csv('employee_data.csv', index=False)\n",
        "\n",
        "print(\"Data collection and saving complete.\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P5rjlclf9-pb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to scrape Google Scholar search results\n",
        "def scrape_google_scholar(keyword, num_articles):\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    params = {\n",
        "        \"q\": keyword,\n",
        "        \"as_ylo\": 2013,\n",
        "        \"as_yhi\": 2023,\n",
        "    }\n",
        "\n",
        "    articles = []\n",
        "\n",
        "    while len(articles) < num_articles:\n",
        "        response = requests.get(base_url, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            results = soup.find_all(\"div\", class_=\"gs_ri\")\n",
        "\n",
        "            for result in results:\n",
        "                article = {}\n",
        "                title_elem = result.find(\"h3\", class_=\"gs_rt\")\n",
        "                if title_elem:\n",
        "                    article[\"Title\"] = title_elem.text\n",
        "\n",
        "                venue_elem = result.find(\"div\", class_=\"gs_a\")\n",
        "                if venue_elem:\n",
        "                    venue_text = venue_elem.text\n",
        "                    article[\"Venue\"] = venue_text\n",
        "\n",
        "                abstract_elem = result.find(\"div\", class_=\"gs_rs\")\n",
        "                if abstract_elem:\n",
        "                    article[\"Abstract\"] = abstract_elem.text\n",
        "\n",
        "                articles.append(article)\n",
        "\n",
        "            # Check if there are more pages of results\n",
        "            next_page = soup.find(\"td\", class_=\"gs_or\")\n",
        "            if not next_page:\n",
        "                break\n",
        "            params[\"start\"] = len(articles)\n",
        "\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return articles[:num_articles]\n",
        "\n",
        "# Parameters\n",
        "keyword = \"information retrieval\"\n",
        "num_articles_to_collect = 10  # Adjust this to the desired number of articles\n",
        "\n",
        "# Scrape articles\n",
        "articles = scrape_google_scholar(keyword, num_articles_to_collect)\n",
        "\n",
        "# Print the articles\n",
        "for idx, article in enumerate(articles, start=1):\n",
        "    print(f\"Article {idx}:\")\n",
        "    print(f\"Title: {article.get('Title', 'N/A')}\")\n",
        "    print(f\"Venue: {article.get('Venue', 'N/A')}\")\n",
        "    print(f\"Abstract: {article.get('Abstract', 'N/A')}\")\n",
        "    print(\"\\n\" + \"=\" * 30 + \"\\n\")  # Separation line between articles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCQpbJnwTxAB"
      },
      "source": [
        "Do either of the question-4 tasks given below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTCoVVeX9e0p"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "# Twitter API credentials (replace with your own)\n",
        "consumer_key = \"YOUR_CONSUMER_KEY\"\n",
        "consumer_secret = \"YOUR_CONSUMER_SECRET\"\n",
        "access_token = \"YOUR_ACCESS_TOKEN\"\n",
        "access_token_secret = \"YOUR_ACCESS_TOKEN_SECRET\"\n",
        "\n",
        "# Set up Tweepy OAuthHandler\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Create the API object\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Function to collect tweets based on a keyword\n",
        "def collect_tweets(keyword, num_tweets):\n",
        "    collected_data = []\n",
        "\n",
        "    for tweet in tweepy.Cursor(api.search, q=keyword, tweet_mode=\"extended\").items(num_tweets):\n",
        "        user_name = tweet.user.screen_name\n",
        "        posted_time = tweet.created_at\n",
        "        text = tweet.full_text\n",
        "\n",
        "        # Append the data to the list\n",
        "        collected_data.append({\n",
        "            'User_name': user_name,\n",
        "            'Posted_time': posted_time,\n",
        "            'Text': text,\n",
        "        })\n",
        "\n",
        "    return collected_data\n",
        "\n",
        "# Parameters\n",
        "keyword = \"your_keyword_here\"\n",
        "num_tweets_to_collect = 1000\n",
        "\n",
        "# Collect tweets\n",
        "tweets = collect_tweets(keyword, num_tweets_to_collect)\n",
        "\n",
        "# Create a DataFrame from the collected data\n",
        "df = pd.DataFrame(tweets)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOeAr9TJTBgS"
      },
      "source": [
        "Question 4 (10 points):\n",
        "\n",
        "In this task, you are required to identify and utilize online tools for web scraping data from websites without the need for coding, with a specific focus on Parsehub. The objective is to gather data and save it in formats like CSV, Excel, or any other suitable file format.\n",
        "\n",
        "You have to mention an introduction to the tool which ever you prefer to use, steps to follow for web scrapping and the final output of the data collected.\n",
        "\n",
        "Upload a document (Word or PDF File) in the same repository and you can add the link in the ipynb file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N20TjXLmTG1u"
      },
      "outputs": [],
      "source": [
        "# Upload the link to the document here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}